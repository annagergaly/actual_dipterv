% !TeX spellcheck = en_GB
% !TeX encoding = UTF-8
% !TeX program = xelatex
%----------------------------------------------------------------------------
\chapter{Technical background}
%----------------------------------------------------------------------------

	In my work I used the Python programming language both for implementing machine learning solutions and for creating pipelines; downloading, sorting and preparing data, as well as various scripting needs. For running deep learning tasks I have employed the Google Colab\footnote{\url{https://colab.research.google.com/}}, an online compute platform that lets users run Jupyter notebooks with limitations, one of my own laptops, equipped with an Nvidia GTX 1050Ti portable, and for the particularly resource intensive diffusion task\ref{sec:diffusion} I have received help from a corporate compute cluster.
	
	In this chapter I detail the frameworks and packages used in my work, as well as the technical aspects of using and preprocessing fMRI data. 
	%TODO használt verziók valami appendixben?

\section{Used Frameworks and Packages}

	For machine learning tasks I have employed various libraries commonly used for such tasks to avoid reimplementing standard solutions, both for creating the different models for comparison and for data exploration and visualization purposes.
	
	For the purpose of organizing the used Python packages and ensuring a consistent environment I have used a various virtual environment management tools in my work: venv, uv and conda. In these types of projects package management is a common painpoint. Since machine learning is a very active field with new discoveries and optimizations packages are updated quite often. Backwards compatibility is not guaranteed usually and issues of certain packages needing another package in a certain version, but not being labeled accurately come up commonly.
	
	Even when all needed package versions are documented correctly in a project, there can still be problems for people trying to recreate the results. Older package versions often become unsupported and unavailable, and package managers might disallow certain combinations of installations.

	\subsection{Main Deep Learning Framework}
	
	The main base deep learning framework I have used in all of my work is Pytorch. Pytorch is one of the most popular foundational machine learning libraries (alongside TensorFlow) used for a wide variety of applications, such as computer vision or NLP. 
	
	At it's core is the provided tensor computing capabilities and an automatic differentiation system, which enables backpropagation. Moreover, Pytorch also has a neural networks module with a lot of widely used lower level concepts: abstract model class, basic layers (e. g.: fully connected, convolutional), optimizers.
	
	\subsection{Graph Neural Network Frameworks}
	
	It is completely possible to implement graph neural network layers and complete graph networks only using the Pytorch library and I have also done so, but there are also frameworks built to enable users to create these networks in just a few lines. These libraries have implementations of commonly used GNN layers (such as GCN or GAT) and usually have their own format for graph storage. This can speed up model creation greatly but working purely from Pytorch offers the greatest flexibility.
	
	I have tried to specific libraries in my work: DGL and PyG. DGL (Deep Graph Library)is a framework agnostic and scalable solution. It represents graphs with a DGLGraph object which stores all graphs as directed using an edge list. It is also possible to add both edge and node features.
	
	PyG (Pytorch Geometric) was specifically created to enhance Pytorch with better GNN capabilities. It aims to follow the design principles of vanilla Pytorch and provide a good interface to both researchers and first-time users.  
	
	\subsection{Other Machine Learning Tools}
	
	Sklearn (Scikit Learn) is a comprehensive library of machine learning solutions, offering a wide variety of models for classification, regression, clustering and dimensionality reduction, as well as model selection and preprocessing tools.
	
	XGBoost (eXtreme Gradient Boosting) is a gradient boosting library that provides a well optimized and very performant gradient boosting machine implementation.
	
	Denoising-diffusion-pytorch is a package commonly used for experimenting and training diffusion models. It provides an easy interface to train diffusion models with custom networks and enables easy DDIM sampling.
	
	Weights \& Biases (WandB) is a library designed to track machine learning work. By integrating WandB into a project the results of a run can be tracked and hyperparameter sweeps can be ran.
	
	
	\subsection{Supporting packages}
	Numpy is a commonly used computing framework with great features regarding N-dimensional arrays. Most machine learning use it to some degree and are compatible with its formats.
	
	Matplotlib is a powerful plotting and visualisation library for Python. It helps in creating easy to understand and colourful graphs from data. Seaborn is an interface to use matplotlib for complex statistical graphics.
	
	Nibabel and nilearn are specifically neuroimaging libraries: the first one for handling the associated file formats and providing access to neuroimages and the latter for enabling analysis of brain volumes.
	
	Tsfresh is a tool designed to calculate time series characteristics. It also contains methods to evaluate the importance of these features for regression and classification tasks.
	
	
\section{Using fMRI data}

	fMRIs produce a very special four dimensional data mass, that is not easily readable by default in most cases. Fortunately there have been specific data formats and libraries created to enable researchers to transport, view and extract information from these scans.

	\subsection{Data format}
	
	NIFTI (Neuroimaging Informatics Technology Initiative) is a file format created to store brain imaging data\cite{nifti}. It was agreed upon as a replace the previous ANALYZE format, which was widespread but had problems: the main problem, that NIFTI aimed to solve was that there was not adequate information about orientation.
	
	Instead of separate files for meta-information and the actual image (like in ANALYZE), this format allows storage as a single .nii file. Since there are very often large areas of a single colour or masked sections these files can be compressed with great results. Most available datasets are downloadable as nii.gz files.
	
	The first three dimensions are reserved for spatial dimensions ($x$, $y$ and $z$) and the fourth one is for time, $t$. The remaining dimensions from fifth to seventh can be used to store other information. The fifth dimension, however, can still have some predefined uses, such as to store voxel-specific distributional parameters or to hold vector-based data. 
	
	The first 348 bytes (or for the later/current version NIFTI-2 500 bytes) of the file are reserved for the metadata header which is full of information on how to interpret the actual data in the file. Great care has been to taken to make ANALYZE, NIFTI-1 and NIFTI-2 as compatible with each other as possible, while advancing the user experience and representation capabilities of the file format. Nibabel is compatible with all of these formats, providing an easy way to access this data from Python.
	
	
	\subsection{Extracting ROI Values and Connectomes}
	\label{sec:extract}
	
	Using the nilearn package it is possible to load brain atlases and use them to segment the brain scan loaded from a NIFTI file using nibabel. It is also possible to create masks and use only certain parts of the brain/atlas. This allows for the creation of average ROI activation timeseries for each region. This means we end up with a tensor that is number of patients $\times$ timesteps $\times$ number of ROIs in size.
	
	Nilearn also has tools to perform the approximation of connectomes from these timeseries. The ConnectivityMeasure class capable of calculating various types of functional connectivity matrixes on multiple subjects. It can be parameterized with a covariance estimator object for more control over the resulting connectomes.
	